{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19777504-aa34-4216-96ac-6c8646f48857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transform\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2901fa7-bbe8-4ba4-b738-6e2b7156ee38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transform.Compose(\n",
    "    [transform.ToTensor(),\n",
    "     transform.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))]\n",
    ")\n",
    "trainset = torchvision.datasets.CIFAR10(root = './data', train = True, download = True, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be3f68a1-ac9b-4d50-a201-578685f618dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1142a9b2-73ed-48f5-9fbb-e58fe2715079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# num_worker -> 加載batch的現成數量，越多越快。\n",
    "# shuffle -> 每個epoch都會重新調整input data的次序\n",
    "# batch_size -> 是指一次訓練要多少 data進去訓練，之後訓練data_size / batch_size次，就算一個epoch（也就是全部run過一次）。\n",
    "# batch_size 越大，epoch需要越多，因為訓練次數會變少。 epoch像是組數，batch_size像是重量，不能太大，也不能太小。\n",
    "trainloader = torch.utils.data.DataLoader( trainset, batch_size = 4, shuffle = True, num_workers = 2)\n",
    "testset = torchvision.datasets.CIFAR10( root = './data', train = False, download = True, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56c70308-ab83-48e5-8170-6ce3c96996fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05fa0417-7d8c-47e6-a07b-69ae086624ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader( testset, batch_size = 4, shuffle = False, num_workers = 2)\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1d5bcfe-a972-400e-bb40-d711083a5deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd3c51f9-48c2-4d53-aad4-d480e9c1f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() # super的用處就是調用father class的function。在這裡就是使用Father class的__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) # input:3, output:6, kernel:5\n",
    "        self.pool = nn.MaxPool2d(2, 2) # kernel:2, stride:2\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # input:6, output:16, kernel:5\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # in->400, output->120\n",
    "        self.fc2 = nn.Linear(120,84) # in->120, output->84\n",
    "        self.fc3 = nn.Linear(84,10) # in->84, output->10\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3599e44a-5f8d-46f0-96a7-47b1d1ce65a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8583c3ec-810f-4bfb-a4e2-652d0fb1b939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x0000023681AC2AB0>\n"
     ]
    }
   ],
   "source": [
    "print(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3d18291-f7aa-4455-bc05-fb9db6b17fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 什麼是Cross Entropy Loss?\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 什麼是optimizer, 用處是什麼？\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f697ccb3-11c1-4486-a1cc-87e256b492ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vars(optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "00d44bea-da72-4d39-aed4-7b5944e0c204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.303\n",
      "[1,  4000] loss: 2.304\n",
      "[1,  6000] loss: 2.304\n",
      "[1,  8000] loss: 2.303\n",
      "[1, 10000] loss: 2.304\n",
      "[1, 12000] loss: 2.304\n",
      "[2,  2000] loss: 2.303\n",
      "[2,  4000] loss: 2.304\n",
      "[2,  6000] loss: 2.303\n",
      "[2,  8000] loss: 2.303\n",
      "[2, 10000] loss: 2.303\n",
      "[2, 12000] loss: 2.304\n",
      "Finish Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # 獲得輸入的資料跟答案\n",
    "        inputs, labels = data\n",
    "        # print(f\"input = {inputs}\")\n",
    "        # print(f\"label = {labels}\")\n",
    "        # 把梯度先設成 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 正向傳播：傳入訓練資料\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels) # 計算loss值\n",
    "        # 反向傳播\n",
    "        loss.backward()\n",
    "        # 優化\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 秀出現在的狀態資訊\n",
    "        running_loss += loss.item()\n",
    "        if i%2000 == 1999:\n",
    "            print(\"[%d, %5d] loss: %.3f\" %(epoch + 1, i+1, running_loss/2000))\n",
    "            running_loss = 0.0\n",
    "                  \n",
    "print('Finish Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b15115fc-e4d6-4bce-899d-dee0ae26bdb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_MultiProcessingDataLoaderIter' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dataiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(testloader)\n\u001b[1;32m----> 2\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[43mdataiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_MultiProcessingDataLoaderIter' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d65d7-d94a-4bb2-926e-a607f083e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tf-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
